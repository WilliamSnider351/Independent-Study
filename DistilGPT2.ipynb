{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fe8WjRxFWEK9"
   },
   "source": [
    "**Import Libraries and Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kbrTIfSii0ve",
    "outputId": "2941d1a9-0be5-4948-c362-d99e8e732aa1"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VF8qQD9JW3tl"
   },
   "source": [
    "**Load Dataset (DailyDialog)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aR6gSSU2kQfS"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"DeepPavlov/daily_dialog\")\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "valid_dataset = dataset[\"validation\"]  # DeepPavlov version has validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39sZE0piXOOo"
   },
   "source": [
    "**Initialize Tokenizer and Model (DistilGPT-2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "593e688e259646a992742d0660d825a4",
      "c9b1b48b144743d5b081b435676f8659",
      "837eaa480d24420989689bc130d36b6c",
      "7aa2854a97854959a3a1219477b8074c",
      "67653241d4614fa5bd64032372be67dd",
      "516dc08428ac48e3933643830bc7e5ce",
      "a8756deb5de84d368cd74259cff152c6",
      "fe0df7fc348e400b949034b6f7af28b2",
      "8b62d9da76c1435bac7a6da712fac804",
      "e124b378179d4411b7638fa99877d283",
      "a0c2612c27134c2f8acd9b80fb2b563a",
      "a45c4b4b9e894182937e1abc508b83fa",
      "ca29ee6a0bac4d94a672a06a221bc4f9",
      "db8ec54845754a07ad19cdd11bc1e740",
      "5fdd0eeb6d4f45f9b78d640e114bd2c9",
      "332049fc542041718d2ca8d32d28d001",
      "1f34875e46fe41e8a4cfb5e2caee0c04",
      "ee7d17bc49f64721b10beed4b951a97c",
      "61cb1050bc8b4118bff8619203406e03",
      "e728e147983244cdb07399ee515050d5",
      "d98fec2b73b24d6f909862a50a4c1bf8",
      "7eb41f786bc548fba519a5d91fffcab3",
      "03e6af79affa4eccaa510489d581c033",
      "236c5952394b4efebb491f7192bf4e0f",
      "c74dbaa3f3a8413dbfa8b6114c1da450",
      "cb36b2e2d65346eab13c846c06c0aca3",
      "1ce8ea03fa684ee0820b9e52541b169c",
      "911edcda91b8479b99b3fbf698b063e8",
      "831b001f442b4b9fb89887086105fb9a",
      "4ee21e3d9de44359b1545c3d4b223de0",
      "de31cb1de3084e29a75cfe27ef945dda",
      "379c60ffcb84466daf0ef141880a82c6",
      "7955fbbfcd3f46da96673f4f111a0820",
      "9f6f15db4e8c4431aada6bb403871ab9",
      "81963dc4a31342c8ad2f88c074819535",
      "31a1cbb6540a4b49ad70fe86aee10b5f",
      "00815bec8ede4556a2bb42e793946dd9",
      "a95a48254c5c49d99e6c0265142ad093",
      "df3c0cf8f89844f08ee033efde9fec31",
      "fc8abc927eb447e58dde068162a378cd",
      "c6facaf75efa477a9967f7f183387257",
      "4fbbff497dc54aaf93bdc0f1b19abc98",
      "3541a0a743904e01989f1bcf583fb3ca",
      "056a68905a0a49ee91a331b0bebbbe99",
      "46344db163e445d49297137ba04fce6e",
      "09df16337667464c9b91afbdfe4003f3",
      "35ff5877237444dcb661f19f36e156fe",
      "6652edea5169447a8f9bd7f9718166f7",
      "ff16e52d05344807947f4370d9a09263",
      "01444799aa6845d99e83d72fdde55edc",
      "acd9940a14cd44c4b7b99d33f3cb3ce2",
      "bbdfc594c9c44b55acf63ac51ac2bce0",
      "e4cf77142af44558a4c57491115663d1",
      "28a46cfd22864d1b91fc76558aad5127",
      "61cd382c739b41d5b81b633e11ecb92f",
      "f2ab8b0e71394a5fb567fb7afdd3b44c",
      "a706356a802e429daa14ea3c4e6ec2af",
      "66de2dd731104096876bfdc587922eda",
      "cedb9e5ac03d433b850a28ecdcdb274c",
      "474e763cc3f0486c88ca95fb326473aa",
      "10803c38750846e29a08b028e0fb5727",
      "496a7686e8ea4919bb93bad4a5a7ded3",
      "a9ffa4a96a304261a513f71a7b027ee3",
      "3ccc183856974c6194694050b4168a70",
      "b60763726c384947b101dfca3afaaa11",
      "a4feae42fe804ccbba4f6744c3bc206f",
      "76934019498c4171a11df908481defd6",
      "7598409c999d4b548ee0e6fbf13bb753",
      "50f111a8af9648dbbafccafbfc850d8d",
      "b3ab1cdc7dea4fe8ae62c1652c4b2644",
      "f0b17fd2a381403eaeb8c51471ae08e9",
      "d868b24be2d64040a9e9aeeec889fe7d",
      "af6de46fa96e4d25bd31f5b9fbff5df4",
      "13a10d9b0b12405ab0107bd16a5ecf54",
      "c51b63a798de4311b615113f8784f024",
      "32f4a6b1ef00443986608703df249e3a",
      "d2f781e136d242e4af80307000c9c271"
     ]
    },
    "id": "ZYPXEfrvkQ5x",
    "outputId": "d387776f-c8e3-40de-be32-51321e99829f"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Ensure padding token exists\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSs4loUsXVyd"
   },
   "source": [
    "**Model Initialization, Optimizer, and DataLoader Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VVmc4lSawGkm"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Load DistilGPT-2 model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(device)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader   = DataLoader(valid_dataset, batch_size=8)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sy8UrlIPXfJZ"
   },
   "source": [
    "**Preprocess Dataset: Flatten Dialogs into Text Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "2ee290738e2c41b7b5d82e38021fc7b6",
      "1d7b5b57c2c24bb1b03fef9ca6978add",
      "9cdb9dda78c644f5b2bacc4b8e224013",
      "3102d2325fba427cb096825f64d9752d",
      "f1f46c77b09046d79f1a60085847ae88",
      "1679468f3a0a482688ec671d182d91b8",
      "fb2bd9e97c92433594c230adb9a95a3c",
      "0366893f5bb242dd989d79e6b016af5d",
      "357b038ab0aa448992740054d144bdfc",
      "b9635465dca6473aaeed9479830327b1",
      "090c23ef83714f4b89b8e950b6af2a16",
      "5261b2ed594f4db89a1a4f6362baaf71",
      "5157505432cf42079caa785e113d96af",
      "eadfdce5d43a44ed95c87e4098dcfb09",
      "bc4154d07b68484e9c98536117f00936",
      "2e2c54a00ed64ae5bd083b8db224dfd0",
      "e2a33e19f2184d6c80860f37d1db6a17",
      "963ead5faa2f4c0ba3b76d350f2b2a70",
      "b5cea371b6474d38a64bb282e3bd646e",
      "458007c6695f4343ab50faa3247762fb",
      "b9a9aee3ed474744a1f1037a344b34a0",
      "c39303e34d6646158513dbf78bcd1ae1"
     ]
    },
    "id": "UON4dI5D46np",
    "outputId": "4e4230c3-20f9-45f7-bce4-82ded70e8155"
   },
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    example[\"text\"] = \" \".join(example[\"dialog\"])\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess)\n",
    "valid_dataset = valid_dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCI3DLO-WvLx"
   },
   "source": [
    "**Dataset Tokenization and Label Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "4e7d0691301f46a0b9d67fc916c80c42",
      "bf22b1d508a2440e9eb01590bb99c56f",
      "25aae3207cb1419bb72cea40af733cba",
      "17d5ddcfd60f468386017bad2c637f04",
      "181129e1cf21430ebe16b76145e31a1d",
      "68c9ee894bdc458bbcc7087cdb5d69db",
      "03caa2d250154425a2931abe6662817c",
      "e7fbccb79b364ebf972e0c758faac6e7",
      "8074eec0ebdc4f83935f1d062bd45d4c",
      "aa502ea0a1714f12a9cf4d7bb26838b6",
      "d3c6907b9cea49e08fc80f801e1f53d9",
      "5e331d438e4f4185a00b9a0ada7c7a93",
      "7d81e1143c004d4fa3770727892ba163",
      "c471b609627f4c16b839fc59ddc13e70",
      "73019d90b87b425d902ec091805a14b7",
      "d0be64aaed8e4e84994609a07c0e0c04",
      "35b18f48b8d44ac291e9d76c81d98374",
      "045dc38d835e49d8a97925ec5f540c72",
      "8185ff05eca34ab1af8f725fa96eda39",
      "bba21736c3754e9382cf93522a612fa5",
      "0957ca62409543dc9c757a3feee25c09",
      "016d72eece1646f093723364677255ba"
     ]
    },
    "id": "8KhfLho15Ak6",
    "outputId": "fd605506-cd41-4c66-fa2d-c758be3571d6"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    tokens = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "    # Labels = input_ids, with padding masked out\n",
    "    tokens[\"labels\"] = [\n",
    "        (label if label != tokenizer.pad_token_id else -100)\n",
    "        for label in tokens[\"input_ids\"]\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=False,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "tokenized_valid = valid_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=False,\n",
    "    remove_columns=valid_dataset.column_names\n",
    ")\n",
    "\n",
    "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_valid.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14Oer_-TV5Eg"
   },
   "source": [
    "**Preprocess Dataset (flatten dialogs into text)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "46a4d55bbe48476d8e487caf35aeb77c",
      "39d2c8e2d4604855bfadb520c03aeb91",
      "4baf43b513c8461c85129ea6a1531673",
      "35783143216848daa91c77a8221b4c7f",
      "693bce94bfd446ad9697df8b79559dcc",
      "6e397d5dc3c54b9a9a93ee1dc798d24a",
      "5af9f1e86d4044478a2545bfde84c3df",
      "b480ec9fd0a04358972d1e52058722e2",
      "34ecc9b4e9e048d085e13dbd556f4c06",
      "ab39e9aebf824b8d9df57d9a6e52469d",
      "abed218e9c4c4c48a7431b13f5b0938a",
      "5b6530fa16f54b06b2bed7f6ba1d4b4e",
      "89ee9f8ad60544a19cca32fca701cbf7",
      "a8500407fa2e489a805fac59cbd26a36",
      "db85ad30ac764578a937a581f6e456b4",
      "be564162ab2a4b41a06570c49e1bb557",
      "fc3cb131506a4ea18fb75feee3a4ff2e",
      "3286e7b4c23d4f2195fb8c923b309449",
      "891b589e638d4dea9b845efa6077ca06",
      "c599c33e53e34ec1becd2df620c690ac",
      "f495f4693c364a118d09542391348b04",
      "ad9da6a1c112448f8e86e061ed33dfcb"
     ]
    },
    "id": "w354lW62ynJp",
    "outputId": "ad50515c-adb2-411a-cb9b-661be1a5781a"
   },
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    # DailyDialog stores utterances in \"dialog\" (a list of strings)\n",
    "    example[\"text\"] = \" \".join(example[\"dialog\"])\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess)\n",
    "valid_dataset = valid_dataset.map(preprocess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROSeV5RDWlF5"
   },
   "source": [
    "**Tokenize Dataset (input_ids, attention_mask, labels)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "b464542bdad548f082432c39ba93e6c5",
      "32960f44bb144d0fa2765d6d2da10863",
      "82b94e09e9164a188d95db46fd24d7c9",
      "6a929df0a4824928a026a2fbf20a74a6",
      "7977a3597d8e45eda7f187da0b2fb0e4",
      "753cc7b56a1c4b078fe23d4eb6f55b50",
      "03a8036e5fe043a3941ae74fa96053af",
      "4943bfe1234842889cc88099038a0f1a",
      "80b5e8eea0d841f4996a17e0eecae0f1",
      "699830ba8f724b988ef9727444d1add9",
      "f13908f945394851934df6eb9e5462fc",
      "3078bd608f3b4ebb992ff68d3bd6b85e",
      "e5aa9305b23040adb1084564f9dcf063",
      "c9c596f253b149fba7da4829196ce1fc",
      "27b01b264cd444e7a89d8f8806419f7d",
      "d3fdfc99daa84493aff9b35d974535fe",
      "e195a32245564259bb908ddc86e8fa36",
      "637616acf3a54b7e8a499565f21f68d5",
      "3b67427cb1004058bb8e8cdac734e9f0",
      "372a07f35f1d4be583e60b7cc6d5fa40",
      "975ae912277642e1b366fecc97b7c605",
      "f8f8d43e089045bab0282b92dfaf74be"
     ]
    },
    "id": "ijFPkqClyUal",
    "outputId": "e52f8f68-d52b-458b-b56d-ea59bc85c113"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Tokenize a batch of texts\n",
    "    tokens = tokenizer(\n",
    "        examples[\"text\"],              # list of strings\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "    # Create labels with padding masked out\n",
    "    tokens[\"labels\"] = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in ids]\n",
    "        for ids in tokens[\"input_ids\"]\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization to train and valid splits\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "valid_dataset = valid_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=valid_dataset.column_names\n",
    ")\n",
    "\n",
    "# Format for PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "valid_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LhgG-rFVpZD"
   },
   "source": [
    "**Training Loop (with Progress Bars and Perplexity)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qA07cEcHVSvD",
    "outputId": "d838eab3-186d-4076-f515-0d891b53d344"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
    "                          collate_fn=data_collator, pin_memory=True, num_workers=4)\n",
    "val_loader   = DataLoader(valid_dataset, batch_size=32,\n",
    "                          collate_fn=data_collator, pin_memory=True, num_workers=4)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()  # ✅ mixed precision\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "            labels = batch[\"labels\"].to(device, non_blocking=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                total_loss += outputs.loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False):\n",
    "        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        labels = batch[\"labels\"].to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    avg_val_loss = evaluate(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}, \"\n",
    "          f\"Train PPL={math.exp(avg_train_loss):.2f}, Val PPL={math.exp(avg_val_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDJUI6KDSsdd"
   },
   "source": [
    "**Save Fine-Tuned Model and Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4LfT1XD2wPdx",
    "outputId": "482b6069-c505-4bba-a0b3-00df0ee8615b"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"my_model\")\n",
    "tokenizer.save_pretrained(\"my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUxCvpC8WbT4"
   },
   "source": [
    "**Reload Model and Run Interactive Chat Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "id": "yQmL54ZquKdh",
    "outputId": "8f038036-d95e-4fa8-f402-81f887b49da6"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load your fine‑tuned DistilGPT‑2 model\n",
    "model_name = \"my_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
    "\n",
    "# Define a pad token (DistilGPT‑2 has none by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Conversation history scaffold\n",
    "history = \"\"\"You: Hello\n",
    "Bot: Hi there! How can I help you today?\n",
    "\"\"\"\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"quit\", \"exit\"]:\n",
    "        break\n",
    "\n",
    "    # Add the new turn to history\n",
    "    history += f\"You: {user_input}\\nBot:\"\n",
    "\n",
    "    # Encode history\n",
    "    inputs = tokenizer(history, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate only the new reply\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=20,                # keep replies short\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.65,                 # balance creativity & focus\n",
    "        top_k=30,\n",
    "        top_p=0.85,\n",
    "        repetition_penalty=1.2            # discourage repeating phrases\n",
    "    )\n",
    "\n",
    "    # Extract only the newly generated tokens\n",
    "    new_tokens = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    bot_reply = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Stop at first newline or \"You:\" to avoid spillover\n",
    "    for stop in [\"\\n\", \"You:\"]:\n",
    "        if stop in bot_reply:\n",
    "            bot_reply = bot_reply.split(stop)[0].strip()\n",
    "\n",
    "    print(\"Bot:\", bot_reply)\n",
    "\n",
    "    # Append reply to history\n",
    "    history += f\" {bot_reply}\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yx_d8xe0ltIC"
   },
   "source": [
    "**Interactive Chat Loop Using Pre-trained DistilGPT-2 (no fine-tuning)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "id": "vn07q5v9xetL",
    "outputId": "56948a8c-b47b-4b90-ecd6-58d97b29eda5"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained DistilGPT-2 directly from Hugging Face\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
    "\n",
    "# Define a pad token (DistilGPT-2 has none by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Conversation history scaffold\n",
    "history = \"\"\"You: Hello\n",
    "Bot: Hi there! How can I help you today?\n",
    "\"\"\"\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"quit\", \"exit\"]:\n",
    "        break\n",
    "\n",
    "    # Add the new turn to history\n",
    "    history += f\"You: {user_input}\\nBot:\"\n",
    "\n",
    "    # Encode history\n",
    "    inputs = tokenizer(history, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate only the new reply\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=25,                # keep replies short\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,                  # balance creativity & coherence\n",
    "        top_k=40,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2            # discourage loops\n",
    "    )\n",
    "\n",
    "    # Extract only the newly generated tokens\n",
    "    new_tokens = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    bot_reply = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Stop at first newline or \"You:\" to avoid spillover\n",
    "    for stop in [\"\\n\", \"You:\"]:\n",
    "        if stop in bot_reply:\n",
    "            bot_reply = bot_reply.split(stop)[0].strip()\n",
    "\n",
    "    print(\"Bot:\", bot_reply)\n",
    "\n",
    "    # Append reply to history\n",
    "    history += f\" {bot_reply}\\n\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
